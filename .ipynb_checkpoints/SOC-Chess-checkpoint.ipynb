{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d3c4271-2ecd-4f77-a545-5f676136ebe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math\n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33e81e66-43d6-44ec-bffb-239d723a9767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ChessGame:\n",
    "    def __init__(self):\n",
    "        self.action_size = 4096\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return chess.Board()\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        next_state = state.copy()\n",
    "        next_state.push(action)\n",
    "        return next_state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        valid_moves = np.zeros(self.action_size)\n",
    "        for move in state.legal_moves:\n",
    "            valid_moves[self.get_action_index(move)] = 1\n",
    "        return valid_moves\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        next_state = state.copy()\n",
    "        next_state.push(action)\n",
    "        if next_state.is_checkmate():\n",
    "            if state.turn:\n",
    "                return -1, True  # Black wins\n",
    "            else:\n",
    "                return 1, True  # White wins\n",
    "        elif next_state.is_stalemate() or next_state.is_insufficient_material():\n",
    "            return 0, True  # Draw\n",
    "        else:\n",
    "            return 0, False\n",
    "    def is_game_over(self,state):\n",
    "        if state.is_checkmate() or state.is_insufficient_material() or state.is_stalemate():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return not player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        if player:\n",
    "            return state\n",
    "        else:\n",
    "            return state.mirror()\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "        for square, piece in state.piece_map().items():\n",
    "            piece_index = self.get_piece_index(piece)\n",
    "            rank = chess.square_rank(square)\n",
    "            file = chess.square_file(square)\n",
    "            encoded_state[piece_index][rank][file] = 1\n",
    "        return encoded_state\n",
    "\n",
    "    def get_piece_index(self, piece):\n",
    "        piece_type = piece.piece_type\n",
    "        color = piece.color\n",
    "        if piece_type == chess.PAWN:\n",
    "            return 0 if color else 6\n",
    "        elif piece_type == chess.KNIGHT:\n",
    "            return 1 if color else 7\n",
    "        elif piece_type == chess.BISHOP:\n",
    "            return 2 if color else 8\n",
    "        elif piece_type == chess.ROOK:\n",
    "            return 3 if color else 9\n",
    "        elif piece_type == chess.QUEEN:\n",
    "            return 4 if color else 10\n",
    "        elif piece_type == chess.KING:\n",
    "            return 5 if color else 11\n",
    "\n",
    "    def get_action_index(self, move):\n",
    "        from_square = move.from_square\n",
    "        to_square = move.to_square\n",
    "        promotion = move.promotion\n",
    "        action_index = from_square * 64 + to_square\n",
    "        if promotion is not None:\n",
    "            action_index += promotion.piece_type\n",
    "        return action_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6fa9a7b-0c0a-454f-aaa6-97ff507c65f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNetChess(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(12, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 8, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.startBlock(x)\n",
    "        for block in self.backBone:\n",
    "            out = block(out)\n",
    "        policy = self.policyHead(out)\n",
    "        value = self.valueHead(out)\n",
    "        return F.softmax(policy, dim=1), value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dffd6f8-1330-473b-b332-3c4a74f245af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, action=None, parent=None):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "        self.visit_count = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == len(self.state.legal_moves)\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def update(self, value):\n",
    "        self.value += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "    def get_ucb_score(self, exploration_constant):\n",
    "        if self.visit_count == 0:\n",
    "            return math.inf\n",
    "        return (self.value / self.visit_count) + exploration_constant * math.sqrt(\n",
    "            math.log(self.parent.visit_count) / self.visit_count\n",
    "        )\n",
    "\n",
    "    def select_best_child(self, exploration_constant):\n",
    "        best_score = float(\"-inf\")\n",
    "        best_child = None\n",
    "\n",
    "        for child in self.children:\n",
    "            score = child.get_ucb_score(exploration_constant)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, model,args):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args=args\n",
    "\n",
    "    def search(self, state, player):\n",
    "        root = Node(state)\n",
    "        for _ in range(self.args[\"num_searches\"]):\n",
    "            leaf = self._traverse(root)\n",
    "            value = self._simulate(leaf.state)\n",
    "            self._backpropagate(leaf, value)\n",
    "        best_child = root.select_best_child(0)\n",
    "        return best_child.action\n",
    "\n",
    "    def _traverse(self, node):\n",
    "        while node.is_fully_expanded():\n",
    "            node = node.select_best_child(self.args[\"exploration_constant\"])\n",
    "        if not node.state.is_game_over():\n",
    "            action = self._select_unexplored_action(node)\n",
    "            next_state = self.game.get_next_state(node.state, action)\n",
    "            child_node = Node(next_state, action, node)\n",
    "            node.add_child(child_node)\n",
    "            return child_node\n",
    "        return node\n",
    "\n",
    "    def _select_unexplored_action(self, node):\n",
    "        valid_moves = self.game.get_valid_moves(node.state)\n",
    "        explored_actions = [child.action for child in node.children]\n",
    "        unexplored_actions = np.setdiff1d(valid_moves, explored_actions)\n",
    "        return np.random.choice(unexplored_actions)\n",
    "\n",
    "    def _simulate(self, state):\n",
    "        current_player = state.turn\n",
    "        while not state.is_game_over():\n",
    "            action = self._select_random_action(state)\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            current_player = self.game.get_opponent(current_player)\n",
    "        value = self.game.get_opponent_value(self.game.get_value_and_terminated(state, action)[0])\n",
    "        return value\n",
    "\n",
    "    def _select_random_action(self, state):\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        valid_actions = np.where(valid_moves == 1)[0]\n",
    "        return np.random.choice(valid_actions)\n",
    "\n",
    "    def _backpropagate(self, node, value):\n",
    "        while node is not None:\n",
    "            node.update(value)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            node = node.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e367543c-2e20-4162-b9b7-6b679471167d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlphaZeroChess:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        memory = []\n",
    "        state = self.game.get_initial_state()\n",
    "        player=chess.WHITE\n",
    "        while not self.game.is_game_over(state):\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.get_action_size(), p=temperature_action_probs)\n",
    "\n",
    "            state = self.game.get_next_state(state, action)\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "        return memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batch_idx in range(0, len(memory), self.args['batch_size']):\n",
    "            batch = memory[batch_idx:batch_idx + self.args['batch_size']]\n",
    "            states, policy_targets, value_targets = zip(*batch)\n",
    "\n",
    "            states = np.array(states)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            out_policy, out_value = self.model(states)\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "\n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iterations']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ae80175-9a84-47c3-a8f8-32d7d95dac96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bd1d13d3be4408a64d91c22986bcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f0cb9f9f9742d48701f719b741a2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d28b57186b4b558d36ec3db45558e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b1f45cfb89467a837fdbe3d1d0ddea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2516e3af54a4997982d5a103834c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faefb089692b410ab54ba4f969cbc879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = ChessGame()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNetChess(tictactoe, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_self_play_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'exploration_constant':1.25\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroChess(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa4463-a592-4e2f-b72b-c9b63f8ae3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8751e-43ee-438f-b73d-f79f44f70489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
